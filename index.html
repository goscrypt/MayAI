<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Local RAG Chatbot (File-Protocol)</title>
  <style>
    body { font-family: sans-serif; max-width: 600px; margin: 2em auto; }
    input, textarea { width: 100%; margin-bottom: 1em; }
    button { padding: .5em 1em; margin-right: .5em; }
    #response { white-space: pre-wrap; background: #fafafa; padding: 1em; border-radius: 4px; }
  </style>
</head>
<body>
  <h2>ðŸ§  RAG Chatbot (File:// â€“ No Server)</h2>

  <input type="file" id="fileInput" accept=".txt" />
  <textarea id="query" rows="3" placeholder="Ask a questionâ€¦"></textarea>
  <button id="go">Generate Answer</button>
  <button id="clear">ðŸ§¹ Clear Cache</button>
  <div id="response">Load a .txt file to get started.</div>

  <script type="module">
    import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@latest/dist/transformers.esm.js';

    // State
    let chunks = [];
    let embeddings = [];
    let llmPipeline;

    // 1. Split text into ~200-word chunks
    function chunkText(text, size = 200) {
      const words = text.split(/\s+/);
      const out = [];
      for (let i = 0; i < words.length; i += size) {
        out.push(words.slice(i, i + size).join(' '));
      }
      return out;
    }

    // 2. Embed chunks with explicit Hugging Face paths
    async function embedChunks(chunks) {
      const embedder = await pipeline('feature-extraction', {
        model: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/config.json',
        tokenizer: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/tokenizer.json',
        tokenizerConfig: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/tokenizer_config.json',
        onnx: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/onnx/model_quantized.onnx'
      });
      const vecs = [];
      for (const c of chunks) {
        const out = await embedder(c);
        vecs.push(out[0]);
      }
      return vecs;
    }

    // 3. Cosine similarity
    function cosine(a, b) {
      let dot = 0, magA = 0, magB = 0;
      for (let i = 0; i < a.length; i++) {
        dot   += a[i] * b[i];
        magA  += a[i] * a[i];
        magB  += b[i] * b[i];
      }
      return dot / (Math.sqrt(magA) * Math.sqrt(magB));
    }

    // 4. RAG query + generate
    async function runRAG() {
      const q = document.getElementById('query').value.trim();
      if (!q || embeddings.length === 0) return;

      // embed the question
      const qEmbedder = await pipeline('feature-extraction', {
        model: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/config.json',
        tokenizer: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/tokenizer.json',
        tokenizerConfig: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/tokenizer_config.json',
        onnx: 'https://huggingface.co/Xenova/nomic-embed-text-v1/resolve/main/onnx/model_quantized.onnx'
      });
      const qVec = (await qEmbedder(q))[0];

      // pick top-3 chunks
      const scores = embeddings.map((v,i) => ({i, s: cosine(v, qVec)}));
      const top3   = scores.sort((a,b) => b.s - a.s).slice(0,3).map(x => x.i);
      const context= top3.map(i => chunks[i]).join('\n');

      // lazy-load LLM with explicit paths
      if (!llmPipeline) {
        llmPipeline = await pipeline('text-generation', {
          model: 'https://huggingface.co/Xenova/distilGPT2/resolve/main/config.json',
          tokenizer: 'https://huggingface.co/Xenova/distilGPT2/resolve/main/tokenizer.json',
          tokenizerConfig: 'https://huggingface.co/Xenova/distilGPT2/resolve/main/tokenizer_config.json',
          onnx: 'https://huggingface.co/Xenova/distilGPT2/resolve/main/onnx/model_quantized.onnx'
        });
      }

      const prompt = `Answer based on:\n${context}\nQuestion: ${q}`;
      const out    = await llmPipeline(prompt, { max_new_tokens: 100 });
      document.getElementById('response').innerText = out[0].generated_text;
    }

    // File upload â†’ chunk, embed, cache
    document.getElementById('fileInput').addEventListener('change', async e => {
      const f = e.target.files[0];
      if (!f) return;
      const txt = await f.text();
      chunks     = chunkText(txt);
      embeddings = await embedChunks(chunks);
      localStorage.setItem('rag_chunks',     JSON.stringify(chunks));
      localStorage.setItem('rag_embeddings', JSON.stringify(embeddings));
      document.getElementById('response').innerText = 'âœ… File loaded & cached.';
    });

    // Clear cache
    document.getElementById('clear').onclick = () => {
      localStorage.removeItem('rag_chunks');
      localStorage.removeItem('rag_embeddings');
      chunks = embeddings = [];
      document.getElementById('response').innerText = 'ðŸ§¹ Cache cleared.';
    };

    // Restore on load
    window.addEventListener('load', () => {
      const c = localStorage.getItem('rag_chunks');
      const e = localStorage.getItem('rag_embeddings');
      if (c && e) {
        chunks     = JSON.parse(c);
        embeddings = JSON.parse(e);
        document.getElementById('response').innerText = 'ðŸ“¦ Cached data loaded.';
      }
    });
  </script>
</body>
</html>
